## Overview

* The algorithms related to this homework are implemented with pure object oriented programming (OOP) and are blackformatted.

* Every algorithm mentioned in the homework assignment is implemented as diligently as possible. Code is completely modular, except for semi convergence task. Though, this can be treated as an exception as running the experiment does not require modularization.

* BB1 & BB2 steps seem to be implemented correctly but the output is deterioriated, this is currently under inspection.

* All implementation is available under [hw04](https://gitlab.lrz.de/IP/teaching/applied-optimization-methods-for-inverse-problems/aomip-kaan-guney-keklikci/-/tree/main/homework/hw04). Specifically, this homework I introduced the superclass `optimize.Optimization`, which implements common ground for iterative reconstruction algorithms, such as their `objectives`, `gradients` as well as an `optimize()` method decorated by `@abstractmethod`. It also composes (credits @david.frank) the `ram_lak` filter.

* Owing to this structural change, some copy/paste programming approaches can now be avoided. 

* Parametrized array callbacks are implemented and passed to `optimize()` for convergence analysis.

* One thing to note is that, although all implementation related to this homework are modular, relevant files are not moved to `aomip` yet. Plan is to modularize the whole source code of the repository by abstracting away some idle functionals attributed to previous homeworks. After this is done, all concrete implementations will be available in `aomip` as always.

* Please find all images in [images](https://gitlab.lrz.de/IP/teaching/applied-optimization-methods-for-inverse-problems/aomip-kaan-guney-keklikci/-/tree/main/homework/hw04/images/) directory as of the date of this release. I am considering to put all my images in `README.md` of the corresponding assignment, but sometimes the scaling is off or the images are offset and quality is deterioriated. For now, please evaluate my work by referencing the images in the specified directory. 

* Also, not all images make sense as they are simply not generated by the right parameters, but nonetheless as part of this work, are included in the repository, especially to correlate reconstruction quality with callback values.

* Please execute the scripts from the [hw04](https://gitlab.lrz.de/IP/teaching/applied-optimization-methods-for-inverse-problems/aomip-kaan-guney-keklikci/-/tree/main/homework/hw04) directory. Note that a reconstructed ground truth image (credits to @david.frank), called `htc2022_04b_recon.tif` must be placed inside `images` as the input image to the algorithms.

* As a final note, some discussion on `Huber`, `Tikhonov` and `Fair Potential` is skipped deliberately as I was not able to hyperparams that outperform this homeworks algorithms and the regular gradient descent regarding those methods. This is in experimentation and I will keep my work updated progressively.

## Homework 1: Even more gradient based methods

  - ### i) Backtracking
    - Unlike other homeworks, python commands are intentionally skipped in these sections. Every script is self explanatory and follows a name convention to match the task at hand. You can then execute these scripts in the order you wish. 
    
    - I implemented and optimized this algorithm by linespacing it over an interval, where then reconstructed images and errors can be plotted.
    
    - The selected range for this experiment was `np.linspace(0.0075, 0.011, num=5)`. Specific values are marked on the plots, please further see `images`.
    
    - Convergence analysis on backtracking shows that `alpha = 0.0075` is the only value that outputs a decent reconstruction. When callback graphs are examined, frequent oscillation of error values indicated a bad reconstruction, which is however not the case for `alpha = 0.0075`.
    
  - ### ii) BB1 (Barzilai - Borwein Steps)
    - As mentioned in the `Overview` section, a healthy discussion cannot be fostered over this algorithm now, with respect to my own implementation. Currently, the approach is to select a random `alpha = 1e-3` and descend by that for the first iteration to avoid division by zero in the first iteration.
    - Nonetheless, I included the output images in the relevant folder as a remark to myself in order to fix it. 
    - Current approach is to fix the export, as the optimization algorithm itself looks fine.
    - As an answer to the convergence rate, longer Barzilai - Borwein step, of course, converges faster compared to the shorter one.
    
  - ### iii) Iterative Shrinkage-Thresholding Algorithm
    - Experimentation with `ISTA` required linespacing over a range of values for both `alphas = np.linspace(1e-5, 1e-3, num=5)` and `betas = np.linspace(1e-8, 1e-6, num=5)`.
    - The values are then zipped and sent to the optimization function for reconstruction and convergence analysis.
    - Looking at the callbacks, we can see that the only decent parameter combination that produces to output a decent reconstruction is `alpha = 1e-5` and `beta = 1e-8`. Whenever the callback values start pulling away from a nicely looking decaying curve, reconstruction is naturally deterioriated.
    - Overall, I found out this algorithm is more sensitive to the parameter `alpha`. A small floating increase or decrease in alpha can in fact completely mess up the reconstruction while `beta` is less concerning, but shouldn't be kept to a very small value either.
    - There are some artifacts on the reconstruction output. I believe this is due to the fact that algortihm is extremely susceptible to the parameter choice, a lot more than the regular gradient descent. Convergence happens halfway, i.e., `50` through the total number of iterations, i.e., `100`. 
    - Backtracking on the other hand, converges `2` times faster w.r.t. the individual optimization parameters of algorithms.

    
## Homework 2: Semi-Convergence

- Initial guess was to to go with vanilla gradient descent because complexity of the algorithm does not necessarily mean that it outperforms the baseline methods.

- Vanilla gradient descent definitely exhibits semi-convergence, we can confidently say that this is evident from convergence analysis graphs. 

- For my experiment, I chose the `Shepp Logan` phantom and used different levels of `Gaussian` noise on the sinogram. 

- For that, I linespaced over a range of values for means and stddevs, i.e., `locs = np.linspace(0, 2, num=5)` and `scales = np.linspace(0, 6, num=5)`.

- Results are just as expected. Until some iterations, we are actually able to reach to a regularized result, at roughly `50` iterations. After that, reconstruction error starts spiking up, where we deviate from the solution. Please specifically see this [error plot](https://gitlab.lrz.de/IP/teaching/applied-optimization-methods-for-inverse-problems/aomip-kaan-guney-keklikci/-/tree/main/homework/hw04/images/semi_convergence_error2.png) as this perfectly demonstrates semi convergence.
    
